{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "# note these notebooks only work inside of .devcontainer\n",
    "\n",
    "# '''create new ipython kernel if needed'''\n",
    "# !python -m ipykernel install --user --name=axonenv\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/app/extract_features/')\n",
    "from helper_functions import get_list_of_h5_files, process_csv_to_dict\n",
    "os.chdir('/data')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_list_of_h5_files(h5_parent_dirs, allowed_scan_types=None, **kwargs):\n",
    "    if allowed_scan_types is None:\n",
    "        allowed_scan_types = kwargs.get('sorting_params', {}).get('allowed_scan_types', [''])[0]\n",
    "    \n",
    "    h5_files = []\n",
    "    for h5_parent_dir in h5_parent_dirs:\n",
    "        if h5_parent_dir.endswith('.h5') and allowed_scan_types in h5_parent_dir:\n",
    "            h5_files.append(h5_parent_dir)\n",
    "            continue\n",
    "        for root, dirs, files in os.walk(h5_parent_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.h5') and allowed_scan_types in root:\n",
    "                    h5_files.append(os.path.join(root, file))\n",
    "    return h5_files\n",
    "\n",
    "def process_csv_to_dict(df, h5_parent_dirs, allowed_scan_types=None, allowed_RBS_scan_types=None):\n",
    "    # Initialize the dictionary to hold the structured data\n",
    "    data_dict = defaultdict(dict)\n",
    "    \n",
    "    # Get list of h5 files from the directories\n",
    "    h5_files = get_list_of_h5_files(h5_parent_dirs, allowed_scan_types, allowed_RBS_scan_types=allowed_RBS_scan_types)\n",
    "    \n",
    "    unmatched_rows = []  # List to track rows that couldn't be matched\n",
    "\n",
    "    # Loop over each row in the DataFrame\n",
    "    lowest_run_num = 999999\n",
    "    lowest_run_id = 41092384\n",
    "    for _, row in df.iterrows():\n",
    "        # Convert the date to YYMMDD format\n",
    "        date_str = dt.datetime.strptime(row['Date'], '%m/%d/%Y').strftime('%y%m%d')\n",
    "        chip_id = row['ID']\n",
    "        RBS_scan_type = row['Assay']\n",
    "        source = row['Neuron Source'].split(', ')\n",
    "        run_number = row['Run #']\n",
    "        run_DIV = row['DIV']\n",
    "        # if run_number < lowest_run_num:\n",
    "        #     lowest_run_num = run_number\n",
    "        # run_num_diff = run_number - lowest_run_num\n",
    "\n",
    "        #filter RBS_scan_type\n",
    "        if allowed_RBS_scan_types is not None:\n",
    "            if RBS_scan_type not in allowed_RBS_scan_types:\n",
    "                continue\n",
    "        \n",
    "        # Filter relevant h5 files for this chip_id and date\n",
    "        relevant_files = [f for f in h5_files if chip_id in f and date_str in f]\n",
    "        \n",
    "        if not relevant_files:\n",
    "            # If no relevant files found, add row to unmatched list and continue\n",
    "            unmatched_rows.append(row)\n",
    "            continue\n",
    "\n",
    "        matched = False  # Flag to check if we successfully matched a file\n",
    "        \n",
    "        for h5_file in relevant_files:\n",
    "            # Extract the scan type from the path, it should be the directory name before the chip_id\n",
    "            maxwell_scan_type = h5_file.split('/')[-3]\n",
    "            \n",
    "            if maxwell_scan_type in allowed_scan_types:\n",
    "                # Extract the run ID from the path (6-digit number in the Network folder)\n",
    "                run_id_match = re.search(r'/(\\d{6})/data\\.raw\\.h5', h5_file)\n",
    "                \n",
    "                # if not run_id_match:\n",
    "                #     continue  # If we can't find the run_id, skip this file\n",
    "\n",
    "                run_id = run_id_match.group(1)\n",
    "                if int(run_id) < int(lowest_run_id):\n",
    "                    lowest_run_id = run_id\n",
    "                run_id_diff = int(run_id) - int(lowest_run_id)\n",
    "                \n",
    "                try:\n",
    "                    assert chip_id in h5_file, f\"{chip_id} not in {h5_file}\"  # Assert that the chip_id is in the h5_file path\n",
    "                    assert date_str in h5_file, f\"{date_str} not in {h5_file}\" # Assert that the date_str is in the h5_file path\n",
    "                    \n",
    "                    #run number will be continuous while run_id may not be if there are any cancelled runs\n",
    "                    assert int(run_number) <= int(run_id), f\"{run_number} not less than {run_id}\" # Assert that the run_number is less than or equal to the run_id\n",
    "                    \n",
    "                    #Rules for matching scan types\n",
    "                    # If the scan type is 'Network', it should be present in the allowed_scan_types\n",
    "                    if 'Network' in allowed_scan_types:\n",
    "                        assert 'Network' in maxwell_scan_type\n",
    "                        try: assert 'Network Today' in RBS_scan_type\n",
    "                        except: assert 'Neuronal Unit' in RBS_scan_type\n",
    "                except AssertionError as e:  # If the assertions fail, print the error and continue to the next file\n",
    "                    print(e)\n",
    "                    continue\n",
    "                \n",
    "                # Create a unique chip identifier based on the run ID (e.g., M08018_000120)\n",
    "                chip_id_with_run = f\"{chip_id}_{run_number}\"\n",
    "                \n",
    "                # Insert the structured data into the dictionary\n",
    "                if date_str not in data_dict:\n",
    "                    data_dict[date_str] = {}\n",
    "                \n",
    "                data_dict[date_str][chip_id_with_run] = {\n",
    "                    \"path\": h5_file,\n",
    "                    \"scan_type\": maxwell_scan_type,\n",
    "                    \"RBS_scan_type\": RBS_scan_type,\n",
    "                    \"DIV\": run_DIV,\n",
    "                    \"source\": source\n",
    "                }\n",
    "                matched = True\n",
    "                break  # If a match is found, no need to check further\n",
    "\n",
    "        if not matched:\n",
    "            unmatched_rows.append(row)\n",
    "\n",
    "    # If there are any unmatched rows, print them out\n",
    "    if unmatched_rows:\n",
    "        print(\"Unmatched rows:\")\n",
    "        for row in unmatched_rows:\n",
    "            print(row)\n",
    "    \n",
    "    return dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 not less than 000008\n",
      "13 not less than 000010\n",
      "13 not less than 000011\n",
      "13 not less than 000012\n",
      "36 not less than 000025\n",
      "36 not less than 000026\n",
      "36 not less than 000027\n",
      "36 not less than 000028\n",
      "36 not less than 000033\n",
      "36 not less than 000034\n",
      "36 not less than 000035\n",
      "49 not less than 000045\n",
      "49 not less than 000047\n",
      "49 not less than 000048\n",
      "63 not less than 000058\n",
      "63 not less than 000060\n",
      "63 not less than 000061\n",
      "63 not less than 000062\n",
      "77 not less than 000072\n",
      "77 not less than 000074\n",
      "77 not less than 000075\n",
      "77 not less than 000076\n",
      "95 not less than 000090\n",
      "95 not less than 000092\n",
      "95 not less than 000093\n",
      "110 not less than 000105\n",
      "110 not less than 000107\n",
      "110 not less than 000108\n",
      "110 not less than 000109\n",
      "127 not less than 000122\n",
      "127 not less than 000124\n",
      "127 not less than 000125\n",
      "127 not less than 000126\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json #save as json, save the json so I can easily read it in vscode\n",
    "file_path = '/app/extract_features/CDKL5-E6D_T1_C1_05152024/CDKL5-E6D_T1_C1_05152024.csv'\n",
    "data = pd.read_csv(file_path) # Load the CSV file to inspect its contents\n",
    "h5_parent_dirs = [\n",
    "    '/data/rbs_maxtwo/rbsmaxtwo/media/rbs-maxtwo/harddisk20tb/CDKL5-E6D_T1_C1_05152024/CDKL5-E6D_T1_C1_05152024',\n",
    "    #'/data/CDKL5-E6D_T1_C1_05152024/CDKL5-E6D_T1_C1_05152024'\n",
    "    ]\n",
    "allowed_scan_types = 'Network'\n",
    "allowed_RBS_scan_types = 'Neuronal Units 9'\n",
    "extracted_data = process_csv_to_dict(data, h5_parent_dirs, allowed_scan_types, allowed_RBS_scan_types=allowed_RBS_scan_types)\n",
    "with open('/app/extract_features/CDKL5-E6D_T1_C1_05152024/extracted_data.json', 'w') as json_file:\n",
    "    json.dump(extracted_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

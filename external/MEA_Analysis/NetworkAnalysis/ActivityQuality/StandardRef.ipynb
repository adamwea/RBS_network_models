{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version for simple check data processing\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_activity_recording_to_csv(base_path, excel_filename):\n",
    "    # Load the Excel file\n",
    "    excel_path = os.path.join(base_path, excel_filename)\n",
    "    sheets = pd.read_excel(excel_path, sheet_name=None)  # Load all sheets into a dictionary\n",
    "\n",
    "    # Process each sheet\n",
    "    for sheet_name, simple_check_df in sheets.items():\n",
    "        # Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "        active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()\n",
    "\n",
    "        # Extract the general information data and the active area data separately, making a copy to avoid SettingWithCopyWarning\n",
    "        general_info_df = simple_check_df.iloc[:active_area_start_idx].copy()\n",
    "        active_area_df = simple_check_df.iloc[active_area_start_idx + 1:].copy()  # skip the row with headers and make a copy\n",
    "        active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "        # Construct correct keys for the mapping\n",
    "        general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "        well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "        # Create the new DataFrame structured as per the requirements\n",
    "        new_csv_data = []\n",
    "        for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "            for index, row in active_area_df.iterrows():\n",
    "                if column in well_plate_genotype_mapping.index:\n",
    "                    plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "                    genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "                    well_number = int(column.split('W')[1])  # Assuming well numbers are like 'W1', 'W2', etc.\n",
    "                    plate_number = general_info_df[general_info_df['Mapping_Key'] == column]['Plate #'].values[0]  # Get the Plate #\n",
    "                    # Standardize NeuronType for any genotype containing \"WT\"\n",
    "                    neuron_type = \"WT\" if \"WT\" in genotype else genotype\n",
    "                    # neuron_type = genotype\n",
    "                    new_csv_data.append({\n",
    "                        'DIV': row['DIV'],\n",
    "                        'Chip_ID': plate_id,\n",
    "                        'Well': well_number,\n",
    "                        'Plate_ID': plate_number,  # Add Plate_ID from the general info\n",
    "                        'NeuronType': neuron_type,\n",
    "                        'Active_area': row[column]\n",
    "                    })\n",
    "\n",
    "        # Convert list of dictionaries into a DataFrame\n",
    "        new_csv_df = pd.DataFrame(new_csv_data)\n",
    "\n",
    "        # Sort the DataFrame by 'DIV', 'Plate_ID', and 'Well'\n",
    "        new_csv_df['Well'] = pd.to_numeric(new_csv_df['Well'])  # Ensure 'Well' is an integer\n",
    "        new_csv_df.sort_values(by=['DIV', 'Plate_ID', 'Well'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "        # Define the full path for saving the file\n",
    "        full_path = os.path.join(base_path, sheet_name, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Save the DataFrame to a new CSV file\n",
    "        new_csv_df.to_csv(full_path, index=False)\n",
    "\n",
    "    print(\"Data processing complete. Files have been saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Files have been saved in their respective directories.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "base_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "excel_filename = 'simple_check.xlsx'\n",
    "process_activity_recording_to_csv(base_path, excel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final version for generate activity and network data for homogenity check\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_and_copy_csvs(source_dir, destination_dir):\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over folders in the source directory\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "            # Define paths for activity and network CSVs\n",
    "            activity_csv_path = os.path.join(folder_path, 'Compiled_ActivityScan.csv')\n",
    "            network_csv_path = os.path.join(folder_path, 'Compiled_Networks.csv')\n",
    "            output_folder = os.path.join(destination_dir, folder)\n",
    "            os.makedirs(output_folder, exist_ok=True)  # Create corresponding folder in destination\n",
    "            \n",
    "            # Process Network CSV\n",
    "            if os.path.exists(network_csv_path):\n",
    "                df_network = pd.read_csv(network_csv_path)\n",
    "                # Standardize 'NeuronType' values and strip spaces\n",
    "                if 'NeuronType' in df_network.columns:\n",
    "                    df_network['NeuronType'] = df_network['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "                # Rename 'IBI' column to 'mean_IBI' if it exists\n",
    "                if 'IBI' in df_network.columns:\n",
    "                    df_network.rename(columns={'IBI': 'mean_IBI'}, inplace=True)\n",
    "                # Rename 'Burst_Peak' column to 'mean_Spike_per_Burst' if it exists\n",
    "                if 'Burst_Peak' in df_network.columns:\n",
    "                    df_network.rename(columns={'Burst_Peak': 'mean_Burst_Peak'}, inplace=True)\n",
    "                # Rename 'Spike_per_Burst' column to 'mean_Spike_per_Burst' if it exists\n",
    "                if 'Spike_per_Burst' in df_network.columns:\n",
    "                    df_network.rename(columns={'Spike_per_Burst': 'mean_Spike_per_Burst'}, inplace=True)\n",
    "                if 'BurstDuration' in df_network.columns:\n",
    "                    df_network.rename(columns={'BurstDuration': 'mean_BurstDuration'}, inplace=True)\n",
    "                # Save the modified CSV to the destination path\n",
    "                df_network.to_csv(os.path.join(output_folder, 'Compiled_Networks.csv'), index=False)\n",
    "            \n",
    "            # Process Activity CSV\n",
    "            if os.path.exists(activity_csv_path):\n",
    "                df_activity = pd.read_csv(activity_csv_path)\n",
    "                # Standardize 'NeuronType' values and strip spaces\n",
    "                if 'NeuronType' in df_activity.columns:\n",
    "                    df_activity['NeuronType'] = df_activity['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "                # Rename 'Active_Electrodes' to 'Active_area' if necessary\n",
    "                if 'Active_area' not in df_activity.columns and 'Active_Electrodes' in df_activity.columns:\n",
    "                    df_activity.rename(columns={'Active_Electrodes': 'Active_area'}, inplace=True)\n",
    "                # Save the modified CSV to the destination path\n",
    "                df_activity.to_csv(os.path.join(output_folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "def merge_activity_data_and_update_networks(homocheck_dir, quickcheck_dir):\n",
    "    homo_folders = os.listdir(homocheck_dir)\n",
    "    quick_folders = os.listdir(quickcheck_dir)\n",
    "\n",
    "    for folder in homo_folders:\n",
    "        homo_path = os.path.join(homocheck_dir, folder)\n",
    "        activity_csv_path = os.path.join(homo_path, 'Compiled_ActivityScan.csv')\n",
    "        network_csv_path = os.path.join(homo_path, 'Compiled_Networks.csv')\n",
    "        \n",
    "        # Update Compiled_Networks.csv if it exists\n",
    "        if os.path.exists(network_csv_path):\n",
    "            df_network = pd.read_csv(network_csv_path)\n",
    "            if 'NeuronType' in df_network.columns:\n",
    "                df_network['NeuronType'] = df_network['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "            df_network.to_csv(network_csv_path, index=False)\n",
    "        \n",
    "        if os.path.exists(activity_csv_path):\n",
    "            df_homo = pd.read_csv(activity_csv_path)\n",
    "            if 'Active_area' not in df_homo.columns and folder in quick_folders:\n",
    "                quick_path = os.path.join(quickcheck_dir, folder, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "                if os.path.exists(quick_path):\n",
    "                    df_quick = pd.read_csv(quick_path)\n",
    "                    if 'Active_area' in df_quick.columns:\n",
    "                        merged_df = pd.merge(df_homo, df_quick[['Well', 'DIV', 'Chip_ID', 'Active_area']],\n",
    "                                             on=['Well', 'DIV', 'Chip_ID'], how='left')\n",
    "                        merged_df.to_csv(activity_csv_path, index=False)\n",
    "\n",
    "def process_datasets(source_dir, destination_dir, quickcheck_dir):\n",
    "    # Adjust and copy datasets\n",
    "    adjust_and_copy_csvs(source_dir, destination_dir)\n",
    "\n",
    "    # Merge additional data into the datasets\n",
    "    merge_activity_data_and_update_networks(destination_dir, quickcheck_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "source = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/CSVs'\n",
    "destination = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/HomoCheck'\n",
    "quickcheck = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "\n",
    "process_datasets(source, destination, quickcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_reffile(file_path, output_path):\n",
    "    # Load the Excel file with all sheets\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    \n",
    "    # Create a writer object to write multiple sheets\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            # Read each sheet, initially loading all columns normally\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "\n",
    "            # Columns to convert to string\n",
    "            str_columns = ['Div', 'Assay', 'Run #', 'Wells_Recorded', 'ID', 'Neuron Source']\n",
    "\n",
    "            # Process each column that needs to be string\n",
    "            for col in str_columns:\n",
    "                if col in df.columns:  # Check if column exists in DataFrame\n",
    "                    # Convert numeric values to string, and ensure no floating point representation\n",
    "                    df[col] = df[col].apply(lambda x: f'{int(x):d}' if pd.notnull(x) and isinstance(x, (int, float)) else x)\n",
    "                    # Strip any leading/trailing white space\n",
    "                    df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "            # Rename 'Div' column to 'DIV' if it exists\n",
    "            if 'Div' in df.columns:\n",
    "                df.rename(columns={'Div': 'DIV'}, inplace=True)\n",
    "\n",
    "            # Save each processed DataFrame to a separate sheet in the same Excel file\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Example usage\n",
    "source_excel_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/Reffiles/selected_Check.xlsx'\n",
    "destination_excel_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/Reffiles/Reffile.xlsx'\n",
    "make_reffile(source_excel_path, destination_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/mtvvz9t52kxgqcx423vlrhx00000gn/T/ipykernel_70775/97710178.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the simple_check.csv to examine its contents\n",
    "simple_check_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/simple_check.csv'\n",
    "simple_check_df = pd.read_csv(simple_check_path)\n",
    "\n",
    "# Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()  # Corrected to specify axis as keyword argument\n",
    "\n",
    "# Extract the general information data and the active area data separately\n",
    "general_info_df = simple_check_df.iloc[:active_area_start_idx]\n",
    "active_area_df = simple_check_df.iloc[active_area_start_idx + 1:]  # skip the row with headers\n",
    "active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "# Construct correct keys for the mapping\n",
    "general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "# Create the new DataFrame structured as per the requirements\n",
    "new_csv_data = []\n",
    "for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "    for index, row in active_area_df.iterrows():\n",
    "        if column in well_plate_genotype_mapping.index:\n",
    "            plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "            genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "            new_csv_data.append({\n",
    "                'DIV': row['DIV'],\n",
    "                'Chip_ID': plate_id,\n",
    "                'Well': column,\n",
    "                'NeuronType': genotype,\n",
    "                'Active_area': row[column]\n",
    "            })\n",
    "\n",
    "# Convert list of dictionaries into a DataFrame\n",
    "new_csv_df = pd.DataFrame(new_csv_data)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "new_csv_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/standardized_simple_check.csv'\n",
    "new_csv_df.to_csv(new_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIV</th>\n",
       "      <th>Chip_ID</th>\n",
       "      <th>Well</th>\n",
       "      <th>NeuronType</th>\n",
       "      <th>Active_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>M05506</td>\n",
       "      <td>P1W1</td>\n",
       "      <td>WT1</td>\n",
       "      <td>51.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>M05506</td>\n",
       "      <td>P1W1</td>\n",
       "      <td>WT1</td>\n",
       "      <td>80.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>M05506</td>\n",
       "      <td>P1W1</td>\n",
       "      <td>WT1</td>\n",
       "      <td>92.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>M05506</td>\n",
       "      <td>P1W1</td>\n",
       "      <td>WT1</td>\n",
       "      <td>95.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>M05506</td>\n",
       "      <td>P1W1</td>\n",
       "      <td>WT1</td>\n",
       "      <td>95.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>18</td>\n",
       "      <td>M07309</td>\n",
       "      <td>P5W3</td>\n",
       "      <td>WT2</td>\n",
       "      <td>93.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>21</td>\n",
       "      <td>M07309</td>\n",
       "      <td>P5W3</td>\n",
       "      <td>WT2</td>\n",
       "      <td>94.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>25</td>\n",
       "      <td>M07309</td>\n",
       "      <td>P5W3</td>\n",
       "      <td>WT2</td>\n",
       "      <td>92.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>27</td>\n",
       "      <td>M07309</td>\n",
       "      <td>P5W3</td>\n",
       "      <td>WT2</td>\n",
       "      <td>90.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>32</td>\n",
       "      <td>M07309</td>\n",
       "      <td>P5W3</td>\n",
       "      <td>WT2</td>\n",
       "      <td>92.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    DIV Chip_ID  Well NeuronType Active_area\n",
       "0     7  M05506  P1W1        WT1       51.05\n",
       "1    11  M05506  P1W1        WT1       80.65\n",
       "2    14  M05506  P1W1        WT1       92.98\n",
       "3    18  M05506  P1W1        WT1       95.61\n",
       "4    21  M05506  P1W1        WT1       95.62\n",
       "..   ..     ...   ...        ...         ...\n",
       "163  18  M07309  P5W3        WT2       93.73\n",
       "164  21  M07309  P5W3        WT2       94.73\n",
       "165  25  M07309  P5W3        WT2       92.56\n",
       "166  27  M07309  P5W3        WT2       90.42\n",
       "167  32  M07309  P5W3        WT2       92.88\n",
       "\n",
       "[168 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/mtvvz9t52kxgqcx423vlrhx00000gn/T/ipykernel_70775/2877308360.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the simple_check.csv to examine its contents\n",
    "simple_check_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/simple_check.csv'\n",
    "simple_check_df = pd.read_csv(simple_check_path)\n",
    "\n",
    "# Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()\n",
    "\n",
    "# Extract the general information data and the active area data separately\n",
    "general_info_df = simple_check_df.iloc[:active_area_start_idx]\n",
    "active_area_df = simple_check_df.iloc[active_area_start_idx + 1:]  # skip the row with headers\n",
    "active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "# Construct correct keys for the mapping\n",
    "general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "# Create the new DataFrame structured as per the requirements\n",
    "new_csv_data = []\n",
    "for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "    for index, row in active_area_df.iterrows():\n",
    "        if column in well_plate_genotype_mapping.index:\n",
    "            plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "            genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "            well_number = column[-1]  # Extract the last character, which is the well number\n",
    "            new_csv_data.append({\n",
    "                'DIV': row['DIV'],\n",
    "                'Chip_ID': plate_id,\n",
    "                'Well': well_number,  # Use the extracted well number\n",
    "                'NeuronType': genotype,\n",
    "                'Active_area': row[column]\n",
    "            })\n",
    "\n",
    "# Convert list of dictionaries into a DataFrame\n",
    "new_csv_df = pd.DataFrame(new_csv_data)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "new_csv_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/standardized_simple_check.csv'\n",
    "new_csv_df.to_csv(new_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hz/mtvvz9t52kxgqcx423vlrhx00000gn/T/ipykernel_70775/3689096833.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the simple_check.csv to examine its contents\n",
    "simple_check_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/simple_check.csv'\n",
    "simple_check_df = pd.read_csv(simple_check_path)\n",
    "\n",
    "# Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()\n",
    "\n",
    "# Extract the general information data and the active area data separately\n",
    "general_info_df = simple_check_df.iloc[:active_area_start_idx]\n",
    "active_area_df = simple_check_df.iloc[active_area_start_idx + 1:]  # skip the row with headers\n",
    "active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "# Construct correct keys for the mapping\n",
    "general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "# Create the new DataFrame structured as per the requirements\n",
    "new_csv_data = []\n",
    "for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "    for index, row in active_area_df.iterrows():\n",
    "        if column in well_plate_genotype_mapping.index:\n",
    "            plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "            genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "            well_number = column[-1]  # Extract the last character, which is the well number\n",
    "            # Standardize NeuronType for any genotype containing \"WT\"\n",
    "            neuron_type = \"WT\" if \"WT\" in genotype else genotype\n",
    "            new_csv_data.append({\n",
    "                'DIV': row['DIV'],\n",
    "                'Chip_ID': plate_id,\n",
    "                'Well': well_number,  # Use the extracted well number\n",
    "                'NeuronType': neuron_type,\n",
    "                'Active_area': row[column]\n",
    "            })\n",
    "\n",
    "# Convert list of dictionaries into a DataFrame\n",
    "new_csv_df = pd.DataFrame(new_csv_data)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "new_csv_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/Compiled_ActivityScan.csv'\n",
    "new_csv_df.to_csv(new_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Files have been saved in their respective directories.\n"
     ]
    }
   ],
   "source": [
    "# # spike only and full trace analysis\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import re  # Import the regular expression library\n",
    "\n",
    "# # Define the base path for saving the results\n",
    "# base_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/SpikeOnly_FullTrace_check'\n",
    "\n",
    "# # Load the Excel file\n",
    "# excel_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/SpikeOnly_FullTrace_check/HET.xlsx'\n",
    "# sheets = pd.read_excel(excel_path, sheet_name=None)  # Load all sheets into a dictionary\n",
    "\n",
    "# # Process each sheet\n",
    "# for sheet_name, simple_check_df in sheets.items():\n",
    "#     # Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "#     active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W4\").any(axis=1)].index.min()\n",
    "\n",
    "#     # Extract the general information data and the active area data separately, making a copy to avoid SettingWithCopyWarning\n",
    "#     general_info_df = simple_check_df.iloc[:active_area_start_idx].copy()\n",
    "#     active_area_df = simple_check_df.iloc[active_area_start_idx + 1:].copy()  # skip the row with headers and make a copy\n",
    "#     active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "#     # Construct correct keys for the mapping\n",
    "#     general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "#     well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "#     # Create the new DataFrame structured as per the requirements\n",
    "#     new_csv_data = []\n",
    "#     for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "#         for index, row in active_area_df.iterrows():\n",
    "#             if column in well_plate_genotype_mapping.index:\n",
    "#                 plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "#                 genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "#                 # Use regex to extract the well number (handles one or two digits)\n",
    "#                 well_number = re.search('W(\\d+)', column).group(1)\n",
    "#                 # Standardize NeuronType for any genotype containing \"WT\"\n",
    "#                 neuron_type = \"WT\" if \"WT\" in genotype else genotype\n",
    "#                 new_csv_data.append({\n",
    "#                     'DIV': row['DIV'],\n",
    "#                     'Chip_ID': plate_id,\n",
    "#                     'Well': well_number,\n",
    "#                     'NeuronType': neuron_type,\n",
    "#                     'Active_area': row[column]\n",
    "#                 })\n",
    "\n",
    "#     # Convert list of dictionaries into a DataFrame\n",
    "#     new_csv_df = pd.DataFrame(new_csv_data)\n",
    "\n",
    "#     # Define the full path for saving the file\n",
    "#     full_path = os.path.join(base_path, sheet_name, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "#     os.makedirs(os.path.dirname(full_path), exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "#     # Save the DataFrame to a new CSV file\n",
    "#     new_csv_df.to_csv(full_path, index=False)\n",
    "\n",
    "# print(\"Data processing complete. Files have been saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path for saving the results\n",
    "base_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = base_path + '/simple_check.xlsx'\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)  # Load all sheets into a dictionary\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, simple_check_df in sheets.items():\n",
    "    # Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "    active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()\n",
    "\n",
    "    # Extract the general information data and the active area data separately, making a copy to avoid SettingWithCopyWarning\n",
    "    general_info_df = simple_check_df.iloc[:active_area_start_idx].copy()\n",
    "    active_area_df = simple_check_df.iloc[active_area_start_idx + 1:].copy()  # skip the row with headers and make a copy\n",
    "    active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "    # Construct correct keys for the mapping\n",
    "    general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "    well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "    # Create the new DataFrame structured as per the requirements\n",
    "    new_csv_data = []\n",
    "    for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "        for index, row in active_area_df.iterrows():\n",
    "            if column in well_plate_genotype_mapping.index:\n",
    "                plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "                genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "                well_number = column[-1]  # Extract the last character, which is the well number\n",
    "                # Standardize NeuronType for any genotype containing \"WT\"\n",
    "                neuron_type = \"WT\" if \"WT\" in genotype else genotype\n",
    "                new_csv_data.append({\n",
    "                    'DIV': row['DIV'],\n",
    "                    'Chip_ID': plate_id,\n",
    "                    'Well': well_number,\n",
    "                    'NeuronType': neuron_type,\n",
    "                    'Active_area': row[column]\n",
    "                })\n",
    "\n",
    "    # Convert list of dictionaries into a DataFrame\n",
    "    new_csv_df = pd.DataFrame(new_csv_data)\n",
    "\n",
    "    # Define the full path for saving the file\n",
    "    full_path = os.path.join(base_path, sheet_name, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    new_csv_df.to_csv(full_path, index=False)\n",
    "\n",
    "print(\"Data processing complete. Files have been saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sorting function\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path for saving the results\n",
    "base_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = base_path + '/simple_check.xlsx'\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)  # Load all sheets into a dictionary\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, simple_check_df in sheets.items():\n",
    "    # Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "    active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()\n",
    "\n",
    "    # Extract the general information data and the active area data separately, making a copy to avoid SettingWithCopyWarning\n",
    "    general_info_df = simple_check_df.iloc[:active_area_start_idx].copy()\n",
    "    active_area_df = simple_check_df.iloc[active_area_start_idx + 1:].copy()  # skip the row with headers and make a copy\n",
    "    active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "    # Construct correct keys for the mapping\n",
    "    general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "    well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "    # Create the new DataFrame structured as per the requirements\n",
    "    new_csv_data = []\n",
    "    for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "        for index, row in active_area_df.iterrows():\n",
    "            if column in well_plate_genotype_mapping.index:\n",
    "                plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "                genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "                well_number = int(column.split('W')[1])  # Assuming well numbers are like 'W1', 'W2', etc.\n",
    "                # Standardize NeuronType for any genotype containing \"WT\"\n",
    "                neuron_type = \"WT\" if \"WT\" in genotype else genotype\n",
    "                new_csv_data.append({\n",
    "                    'DIV': row['DIV'],\n",
    "                    'Chip_ID': plate_id,\n",
    "                    'Well': well_number,\n",
    "                    'NeuronType': neuron_type,\n",
    "                    'Active_area': row[column]\n",
    "                })\n",
    "\n",
    "    # Convert list of dictionaries into a DataFrame\n",
    "    new_csv_df = pd.DataFrame(new_csv_data)\n",
    "    \n",
    "    # Sort the DataFrame by 'DIV' and 'Well' with Well treated as integer\n",
    "    new_csv_df['Well'] = pd.to_numeric(new_csv_df['Well'])  # Ensure 'Well' is an integer\n",
    "    new_csv_df.sort_values(by=['DIV', 'Well'], ascending=[True, True], inplace=True)\n",
    "\n",
    "    # Define the full path for saving the file\n",
    "    full_path = os.path.join(base_path, sheet_name, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    new_csv_df.to_csv(full_path, index=False)\n",
    "\n",
    "print(\"Data processing complete. Files have been saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Files have been saved in their respective directories.\n"
     ]
    }
   ],
   "source": [
    "# add sorting function\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path for saving the results\n",
    "base_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = base_path + '/simple_check.xlsx'\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)  # Load all sheets into a dictionary\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, simple_check_df in sheets.items():\n",
    "    # Identify the index where the second part starts by locating the header \"P1W1\"\n",
    "    active_area_start_idx = simple_check_df[simple_check_df.eq(\"P1W1\").any(axis=1)].index.min()\n",
    "\n",
    "    # Extract the general information data and the active area data separately, making a copy to avoid SettingWithCopyWarning\n",
    "    general_info_df = simple_check_df.iloc[:active_area_start_idx].copy()\n",
    "    active_area_df = simple_check_df.iloc[active_area_start_idx + 1:].copy()  # skip the row with headers and make a copy\n",
    "    active_area_df.columns = simple_check_df.iloc[active_area_start_idx]  # Set new header for active area data\n",
    "\n",
    "    # Construct correct keys for the mapping\n",
    "    general_info_df['Mapping_Key'] = general_info_df.apply(lambda x: f'P{x[\"Plate #\"]}W{x[\"Well #\"]}', axis=1)\n",
    "    well_plate_genotype_mapping = general_info_df.set_index('Mapping_Key')[['Plate ID', 'Genotype']]\n",
    "\n",
    "    # Create the new DataFrame structured as per the requirements\n",
    "    new_csv_data = []\n",
    "    for column in active_area_df.columns[2:]:  # skip 'Date' and 'DIV' columns\n",
    "        for index, row in active_area_df.iterrows():\n",
    "            if column in well_plate_genotype_mapping.index:\n",
    "                plate_id = well_plate_genotype_mapping.loc[column, 'Plate ID']\n",
    "                genotype = well_plate_genotype_mapping.loc[column, 'Genotype']\n",
    "                well_number = int(column.split('W')[1])  # Assuming well numbers are like 'W1', 'W2', etc.\n",
    "                plate_number = general_info_df[general_info_df['Mapping_Key'] == column]['Plate #'].values[0]  # Get the Plate #\n",
    "                # Standardize NeuronType for any genotype containing \"WT\"\n",
    "                neuron_type = \"WT\" if \"WT\" in genotype else genotype\n",
    "                new_csv_data.append({\n",
    "                    'DIV': row['DIV'],\n",
    "                    'Chip_ID': plate_id,\n",
    "                    'Well': well_number,\n",
    "                    'Plate_ID': plate_number,  # Add Plate_ID from the general info\n",
    "                    'NeuronType': neuron_type,\n",
    "                    'Active_area': row[column]\n",
    "                })\n",
    "\n",
    "    # Convert list of dictionaries into a DataFrame\n",
    "    new_csv_df = pd.DataFrame(new_csv_data)\n",
    "    \n",
    "    # Sort the DataFrame by 'DIV', 'Plate_ID', and 'Well'\n",
    "    new_csv_df['Well'] = pd.to_numeric(new_csv_df['Well'])  # Ensure 'Well' is an integer\n",
    "    new_csv_df.sort_values(by=['DIV', 'Plate_ID', 'Well'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "    # Define the full path for saving the file\n",
    "    full_path = os.path.join(base_path, sheet_name, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    new_csv_df.to_csv(full_path, index=False)\n",
    "\n",
    "print(\"Data processing complete. Files have been saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_and_process_csvs(base_path_1, base_path_2, output_path):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Collect all valid folder names from both directions to create corresponding folders in the output directory\n",
    "    all_folders = set()\n",
    "    for base_path in [base_path_1, base_path_2]:\n",
    "        for folder in os.listdir(base_path):\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "                all_folders.add(folder)\n",
    "\n",
    "    # Create folders in the output path\n",
    "    for folder in all_folders:\n",
    "        os.makedirs(os.path.join(output_path, folder), exist_ok=True)\n",
    "\n",
    "    # Process the first direction\n",
    "    for folder in os.listdir(base_path_1):\n",
    "        folder_path = os.path.join(base_path_1, folder)\n",
    "        if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "            # Check for the existence of CSV files\n",
    "            activity_csv = os.path.join(folder_path, 'Compiled_ActivityScan.csv')\n",
    "            network_csv = os.path.join(folder_path, 'Compiled_Networks.csv')\n",
    "            \n",
    "            # Process Activity CSV\n",
    "            if os.path.exists(activity_csv):\n",
    "                df_activity = pd.read_csv(activity_csv)\n",
    "                # Standardize 'NeuronType' values\n",
    "                df_activity['NeuronType'] = df_activity['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "                # Rename column if necessary\n",
    "                if 'Active_Electrodes' in df_activity.columns:\n",
    "                    df_activity.rename(columns={'Active_Electrodes': 'Active_area'}, inplace=True)\n",
    "                \n",
    "                # Save the modified CSV to the output path\n",
    "                df_activity.to_csv(os.path.join(output_path, folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "            # Process Network CSV\n",
    "            if os.path.exists(network_csv):\n",
    "                df_network = pd.read_csv(network_csv)\n",
    "                # Rename 'IBI' column to 'mean_IBI' if it exists\n",
    "                if 'IBI' in df_network.columns:\n",
    "                    df_network.rename(columns={'IBI': 'mean_IBI'}, inplace=True)\n",
    "                \n",
    "                # Save the modified CSV to the output path\n",
    "                df_network.to_csv(os.path.join(output_path, folder, 'Compiled_Networks.csv'), index=False)\n",
    "    \n",
    "    # Process the second direction\n",
    "    for folder in os.listdir(base_path_2):\n",
    "        folder_path = os.path.join(base_path_2, folder)\n",
    "        if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "            activity_folder_path = os.path.join(folder_path, 'Activity')\n",
    "            if os.path.exists(activity_folder_path):\n",
    "                activity_csv = os.path.join(activity_folder_path, 'Compiled_ActivityScan.csv')\n",
    "                if os.path.exists(activity_csv):\n",
    "                    # Read and directly save the CSV to the output directory\n",
    "                    df_activity = pd.read_csv(activity_csv)\n",
    "                    df_activity.to_csv(os.path.join(output_path, folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "# Example usage\n",
    "base_path_1 = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/CSVs'\n",
    "base_path_2 = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "output_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/HomoCheck'\n",
    "\n",
    "combine_and_process_csvs(base_path_1, base_path_2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_and_process_csvs(base_path_1, base_path_2, output_path):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Collect folder names from the CSVs directory (base_path_1) and create corresponding folders in the output directory\n",
    "    valid_folders = [folder for folder in os.listdir(base_path_1) if os.path.isdir(os.path.join(base_path_1, folder)) and not folder.startswith('.')]\n",
    "    for folder in valid_folders:\n",
    "        os.makedirs(os.path.join(output_path, folder), exist_ok=True)\n",
    "\n",
    "    # Process the first direction (CSVs directory)\n",
    "    for folder in valid_folders:\n",
    "        folder_path = os.path.join(base_path_1, folder)\n",
    "        # Check for the existence of CSV files\n",
    "        activity_csv = os.path.join(folder_path, 'Compiled_ActivityScan.csv')\n",
    "        network_csv = os.path.join(folder_path, 'Compiled_Networks.csv')\n",
    "        \n",
    "        # Process Activity CSV\n",
    "        if os.path.exists(activity_csv):\n",
    "            df_activity = pd.read_csv(activity_csv)\n",
    "            # Standardize 'NeuronType' values\n",
    "            df_activity['NeuronType'] = df_activity['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "            # Rename column if necessary\n",
    "            if 'Active_Electrodes' in df_activity.columns:\n",
    "                df_activity.rename(columns={'Active_Electrodes': 'Active_area'}, inplace=True)\n",
    "            \n",
    "            # Save the modified CSV to the output path\n",
    "            df_activity.to_csv(os.path.join(output_path, folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "        # Process Network CSV\n",
    "        if os.path.exists(network_csv):\n",
    "            df_network = pd.read_csv(network_csv)\n",
    "            # Rename 'IBI' column to 'mean_IBI' if it exists\n",
    "            if 'IBI' in df_network.columns:\n",
    "                df_network.rename(columns={'IBI': 'mean_IBI'}, inplace=True)\n",
    "            \n",
    "            # Save the modified CSV to the output path\n",
    "            df_network.to_csv(os.path.join(output_path, folder, 'Compiled_Networks.csv'), index=False)\n",
    "    \n",
    "    # Process the second direction (QuickCheck directory)\n",
    "    for folder in valid_folders:\n",
    "        activity_folder_path = os.path.join(base_path_2, folder, 'Activity')\n",
    "        if os.path.exists(activity_folder_path):\n",
    "            activity_csv = os.path.join(activity_folder_path, 'Compiled_ActivityScan.csv')\n",
    "            if os.path.exists(activity_csv):\n",
    "                # Read and directly save the CSV to the output directory\n",
    "                df_activity = pd.read_csv(activity_csv)\n",
    "                df_activity.to_csv(os.path.join(output_path, folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "# Example usage\n",
    "base_path_1 = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/CSVs'\n",
    "base_path_2 = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "output_path = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/HomoCheck'\n",
    "\n",
    "combine_and_process_csvs(base_path_1, base_path_2, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_and_copy_csvs(source_dir, destination_dir):\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over folders in the source directory\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "            # Define paths for activity and network CSVs\n",
    "            activity_csv_path = os.path.join(folder_path, 'Compiled_ActivityScan.csv')\n",
    "            network_csv_path = os.path.join(folder_path, 'Compiled_Networks.csv')\n",
    "            output_folder = os.path.join(destination_dir, folder)\n",
    "            os.makedirs(output_folder, exist_ok=True)  # Create corresponding folder in destination\n",
    "            \n",
    "            # Process Network CSV\n",
    "            if os.path.exists(network_csv_path):\n",
    "                df_network = pd.read_csv(network_csv_path)\n",
    "                # Rename 'IBI' column to 'mean_IBI' if it exists\n",
    "                if 'IBI' in df_network.columns:\n",
    "                    df_network.rename(columns={'IBI': 'mean_IBI'}, inplace=True)\n",
    "                # Save the modified CSV to the destination path\n",
    "                df_network.to_csv(os.path.join(output_folder, 'Compiled_Networks.csv'), index=False)\n",
    "            \n",
    "            # Process Activity CSV\n",
    "            if os.path.exists(activity_csv_path):\n",
    "                df_activity = pd.read_csv(activity_csv_path)\n",
    "                # Standardize 'NeuronType' values and strip spaces\n",
    "                df_activity['NeuronType'] = df_activity['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "                # Rename 'Active_Electrodes' to 'Active_area' if necessary\n",
    "                if 'Active_area' not in df_activity.columns and 'Active_Electrodes' in df_activity.columns:\n",
    "                    df_activity.rename(columns={'Active_Electrodes': 'Active_area'}, inplace=True)\n",
    "                # Save the modified CSV to the destination path\n",
    "                df_activity.to_csv(os.path.join(output_folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "# Example usage\n",
    "source_dir = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/CSVs'\n",
    "destination_dir = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/HomoCheck'\n",
    "\n",
    "adjust_and_copy_csvs(source_dir, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_and_copy_csvs(source_dir, destination_dir):\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over folders in the source directory\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if os.path.isdir(folder_path) and not folder.startswith('.'):\n",
    "            # Define paths for activity and network CSVs\n",
    "            activity_csv_path = os.path.join(folder_path, 'Compiled_ActivityScan.csv')\n",
    "            network_csv_path = os.path.join(folder_path, 'Compiled_Networks.csv')\n",
    "            output_folder = os.path.join(destination_dir, folder)\n",
    "            os.makedirs(output_folder, exist_ok=True)  # Create corresponding folder in destination\n",
    "            \n",
    "            # Process Network CSV\n",
    "            if os.path.exists(network_csv_path):\n",
    "                df_network = pd.read_csv(network_csv_path)\n",
    "                # Standardize 'NeuronType' values and strip spaces\n",
    "                if 'NeuronType' in df_network.columns:\n",
    "                    df_network['NeuronType'] = df_network['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "                # Rename 'IBI' column to 'mean_IBI' if it exists\n",
    "                if 'IBI' in df_network.columns:\n",
    "                    df_network.rename(columns={'IBI': 'mean_IBI'}, inplace=True)\n",
    "                # Save the modified CSV to the destination path\n",
    "                df_network.to_csv(os.path.join(output_folder, 'Compiled_Networks.csv'), index=False)\n",
    "            \n",
    "            # Process Activity CSV\n",
    "            if os.path.exists(activity_csv_path):\n",
    "                df_activity = pd.read_csv(activity_csv_path)\n",
    "                # Standardize 'NeuronType' values and strip spaces\n",
    "                if 'NeuronType' in df_activity.columns:\n",
    "                    df_activity['NeuronType'] = df_activity['NeuronType'].str.strip().replace(regex={r'^.*WT.*$': 'WT'})\n",
    "                # Rename 'Active_Electrodes' to 'Active_area' if necessary\n",
    "                if 'Active_area' not in df_activity.columns and 'Active_Electrodes' in df_activity.columns:\n",
    "                    df_activity.rename(columns={'Active_Electrodes': 'Active_area'}, inplace=True)\n",
    "                # Save the modified CSV to the destination path\n",
    "                df_activity.to_csv(os.path.join(output_folder, 'Compiled_ActivityScan.csv'), index=False)\n",
    "\n",
    "# Example usage\n",
    "source_dir = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/CSVs'\n",
    "destination_dir = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/HomoCheck'\n",
    "\n",
    "adjust_and_copy_csvs(source_dir, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Activity CSV found in HomoCheck for folder .DS_Store.\n",
      "Updated 'Active_area' for ADNP_T2_10262023.\n",
      "Updated 'Active_area' for ADNP_T3_11072023.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_activity_data(homocheck_dir, quickcheck_dir):\n",
    "    homo_folders = os.listdir(homocheck_dir)\n",
    "    quick_folders = os.listdir(quickcheck_dir)\n",
    "\n",
    "    for folder in homo_folders:\n",
    "        homo_path = os.path.join(homocheck_dir, folder)\n",
    "        activity_csv_path = os.path.join(homo_path, 'Compiled_ActivityScan.csv')\n",
    "        \n",
    "        if os.path.exists(activity_csv_path):\n",
    "            df_homo = pd.read_csv(activity_csv_path)\n",
    "            \n",
    "            if 'Active_area' not in df_homo.columns:\n",
    "                # Check if there is a corresponding folder in QuickCheck\n",
    "                if folder in quick_folders:\n",
    "                    quick_path = os.path.join(quickcheck_dir, folder, 'Activity', 'Compiled_ActivityScan.csv')\n",
    "                    \n",
    "                    if os.path.exists(quick_path):\n",
    "                        df_quick = pd.read_csv(quick_path)\n",
    "                        if 'Active_area' in df_quick.columns:\n",
    "                            # Perform the merge based on 'Well', 'DIV', and 'Chip_ID'\n",
    "                            merged_df = pd.merge(df_homo, df_quick[['Well', 'DIV', 'Chip_ID', 'Active_area']],\n",
    "                                                 on=['Well', 'DIV', 'Chip_ID'], how='left')\n",
    "                            \n",
    "                            if merged_df['Active_area'].isnull().all():\n",
    "                                print(f\"No rows to match in {folder} based on 'Well', 'DIV', and 'Chip_ID'.\")\n",
    "                            else:\n",
    "                                # Save back to HomoCheck folder\n",
    "                                merged_df.to_csv(activity_csv_path, index=False)\n",
    "                                print(f\"Updated 'Active_area' for {folder}.\")\n",
    "                        else:\n",
    "                            print(f\"No 'Active_area' column found in QuickCheck for folder {folder}.\")\n",
    "                    else:\n",
    "                        print(f\"No matching CSV found in QuickCheck for folder {folder}.\")\n",
    "                else:\n",
    "                    print(f\"No matching folder found in QuickCheck for folder {folder}.\")\n",
    "        else:\n",
    "            print(f\"No Activity CSV found in HomoCheck for folder {folder}.\")\n",
    "\n",
    "# Example usage\n",
    "homocheck_dir = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/HomoCheck'\n",
    "quickcheck_dir = '/Users/liufanling/Library/CloudStorage/OneDrive-Personal/1 UC DAVIS/2024 Summer/CSRA/QualityCheck/QuickCheck'\n",
    "\n",
    "merge_activity_data(homocheck_dir, quickcheck_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
